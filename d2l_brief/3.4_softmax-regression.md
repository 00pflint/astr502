# 3.4 Softmax Regression

## 1. Classification Problem

**Ways to represent categorical or discrete target variable  y** <br>


- Label encoding <br>
  e.g. Hand-written digits  <br>
y ∈ { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }

- **One-hot encoding**  <br>
e.g. y ∈ {(1,0,0), (0,1,0), (0,0,1)} <br>
    Useful expression when there's no natural ordering on the categorical target variables. 

## 2. Network Architecture

How many parameters are there in this network structure?

<img src='./images/3.4.1.png' width='500'/>

**w** matrix : 4x3 parameters <br>
**b** bias   : 3 parameters (for 3 neurons in the output layer)

**logits** : $o_1$, $o_2$, $o_3$ (linear weighted sum of the inputs $x_{1-4}$ )

<img src='./images/eq_3.4.2.png' width='600'/>



## 3. Parameterization Cost of Fully-Connected Layers (Optional)

- [Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters](https://arxiv.org/abs/2102.08597) (Zhang et al. 2021) 

## 4. Softmax Operation

<img src='./images/eq_3.4.3.png' width='600'/>

## 5. Vectorization for Minibatches

<img src='./images/eq_3.4.5.png' width='500'/>

&emsp; n : number of training examples per batch <br>
&emsp; d : dimension of input vector  <br>
&emsp; q : dimension of output vector  <br>

<img src='./images/eq_3.4.5_expand.png' width='800'/>


## 6. Cross Entropy Loss Function for Softmax Regression

<img src='./images/slide_cross_entropy_loss.png' width='800'/>

## 7. Information Theory Basics (Optional)